# Data Retrieval

When creating a RAG-enabled chat endpoint, we use the `getDataRetriever` method to obtain a vector store retriever. This may or may not include data ingestion whereby we load the data, generate embeddings and store them in a vector store. To learn about data ingestion, check [Data Ingestion](/rag-guide/data-ingestion).

## Data Retriever

The `getDataRetriever` method is used to obtain a vector store retriever. The method takes in a configuration object with the following properties:

- `dataType`: The type of data loader to use.
- `filePath`: The path to the file containing the data.
- `jsonLoaderKeysToInclude`: The keys to include when loading JSON data.
- `csvLoaderOptions`: The options for loading CSV data.
- `pdfLoaderOptions`: The options for loading PDF data.
- `dataSplitterType`: The type of data splitter to use.
- `chunkingConfig`: The configuration for chunking the data.
- `splitterConfig`: The configuration for the data splitter.
- `retrievalOptions`: The retrieval options for the retriever.
- `vectorStore`: The vector store to use.
- `embeddingModel`: The embedding model to use.
- `generateEmbeddings`: Whether to generate embeddings.

## Retrieving Data

When a RAG-enabled chat endpoint received a query, it uses the provided data retriever to retrieve the relevant context information from the vector store to which the data retriever belongs.

The underlying process is as follows:

1. **Query Embedding Generation**: Each data retriever is attached to a vector store, and a vector store has an embedding model specified for it. This embedding model is used to generate embedding vectors for the given user query.
2. **Vector Store Search**: The generated embedding vectors are used to search the vector store to retrieve the most relevant context information. You can configure your data retriever to use different types of search algorithms, such as similarity matching or Maximal Marginal Relevance (MMR), or specify the number of documents to retrieve. All such configurations will greatly impact the performance of your RAG chat endpoint. We recommend you start with default parameters, but always test with different configurations to find the best one for your use case.
3. **Conversion to String**: The retrieved context information is converted back to a string format so it can be embedded in the query that will be sent to the LLM model. A specific prompt template maybe used to structure the query and context information in a way that the LLM model can understand and generate a meaningful response.
4. **Response Generation**: The query with the context information is sent to the LLM model, which generates a response based on the query and context information.

Below is an example of a custom configured data retriever:

```typescript copy
// Index inventory data and get retriever
const inventoryDataRetriever = await getDataRetriever({
  dataType: "csv",
  filePath: "data/knowledge-bases/inventory-data.csv",
  generateEmbeddings: true,
  retrievalOptions: {
    k: 10, // return top 10 matching documents
    searchType: "mmr", // use Maximal Marginal Relevance (MMR) for search
  },
});
```
